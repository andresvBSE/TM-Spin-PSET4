{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb26976c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (13,14,19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#Path\n",
    "os.chdir(r\"C:\\Users\\lenovo\\Documents\\BSE\\SEM_2\\text_mining\\as_4\")\n",
    "\n",
    "#Read in file\n",
    "\n",
    "corpus_data = pd.read_csv(\"all_english.csv\", delimiter=',', encoding='utf-8', converters = {\"tweet_hashtags\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "corpus_data.head()\n",
    "#%% \n",
    "# Auxiliary functions\n",
    "def cleanTweets(s):\n",
    "    #Line breaks\n",
    "    s = s.replace(r'<lb>', \"\\n\")\n",
    "    s = re.sub(r'<br */*>', \"\\n\", s)\n",
    "    # Tabs\n",
    "    s = s.replace(r'<tab>', \"\\i\")\n",
    "    #Symbols\n",
    "    s = s.replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\").replace(\"&amp;\", \"&\")\n",
    "    s = s.replace(\"&amp;\", \"&\")\n",
    "    # urls\n",
    "    s = re.sub(r'\\(https*://[^\\)]*\\)', \"[url]\", s)\n",
    "    s = re.sub(r'https*://[^\\s]*', \"[url]\", s)\n",
    "    # Replace double instances of quotations with single instance\n",
    "    s = re.sub(r'\"+', '\"', s)\n",
    "    # custom removals\n",
    "    s = re.sub(r'@[A-Za-z0-9_]+', \"@usermention\", s) # remove mentions\n",
    "    #s = re.sub(r'#[A-Za-z0-9_]+', \"#hashtag\", s) # remove hashtags\n",
    "    s = re.sub(r':[^:]+','[emoji]',s) # remove demojized text\n",
    "    return str(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93460e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp/ipykernel_24040/2155958812.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['demojized_text'] = [cleanTweets(text) for text in df['demojized_text']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32438\n",
      "40764\n"
     ]
    }
   ],
   "source": [
    "uk_data = corpus_data[corpus_data[\"group_country\"]==\"United Kingdom\"]\n",
    "uk_cons = uk_data[uk_data[\"party_name\"]==\"Conservative\"]\n",
    "uk_labour = uk_data[uk_data[\"party_name\"]==\"Labour\"]\n",
    "for df in uk_cons, uk_labour:\n",
    "    df['demojized_text'] = [cleanTweets(text) for text in df['demojized_text']]\n",
    "print(len(uk_cons))\n",
    "print(len(uk_labour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5db08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix has size (32438, 199)\n",
      "matrix has size (40764, 465)\n",
      "Duration: 0:00:06.153827\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Add custom stop words\n",
    "add_stopwords = ['usermention','emoji','url'] # can add hashtag here\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stopwords)\n",
    "\n",
    "# Vectorise word counts - only construct tri-grams\n",
    "vec = CountVectorizer(ngram_range = (4,4), stop_words=stop_words, min_df=15, max_df=0.6)\n",
    "#Fit vectoriser and convert to dense matrix\n",
    "uk_vector_cons = vec.fit_transform(uk_cons.demojized_text).todense()\n",
    "uk_vector_lab = vec.fit_transform(uk_labour.demojized_text).todense()\n",
    "# Term frequencies\n",
    "tf_cons = np.array(uk_vector_cons)\n",
    "tf_lab = np.array(uk_vector_lab)# frequencies of each token in a numpy array\n",
    "totaltf_cons = tf_cons.sum(axis=0) # sum of all frequencies for a particular token for all corpus (column)\n",
    "totaltf_lab = tf_lab.sum(axis=0)\n",
    "print(\"matrix has size\", uk_vector_cons.shape)\n",
    "print(\"matrix has size\", uk_vector_lab.shape)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "298d48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['attorney', 'general', 'legal', 'advice'], ['clinically', 'unjustifiable', 'father', 'autistic'], ['consecutive', 'year', 'central', 'government'], ['govt', 'really', 'embarrassing', 'shambles'], ['bid', 'deliver', 'meaningful', 'changes'], ['brexit', 'deal', 'knows', 'lose'], ['check', 'labour', 'personal', 'manifesto'], ['deal', 'brexit', 'nthis', 'acceptable'], ['getting', 'longer', 'public', 'health'], ['issues', 'care', 'll', 'tell'], ['issues', 'challenge', 'boris', 'johnson'], ['attempt', 'make', 'bad', 'deal'], ['deal', 'nshe', 'running', 'scared'], ['chaos', 'unable', 'deliver', 'brexit'], ['damaging', 'impact', 'brexit', 'deal'], ['adverts', 'featured', 'misleading', 'factually'], ['bad', 'isn', 'willing', 'parliament'], ['boris', 'johnson', 'caught', 'lying'], ['botched', 'brexit', 'deal', 'worst'], ['bring', 'forward', 'meaningful', 'vote'], ['care', 'll', 'tell', 've'], ['country', 'puts', 'jobs', 'economy'], ['cuts', 'police', 'simple', 'truth'], ['daily', 'monthly', 'overdraft', 'fees'], ['december', 'save', 'nhs', 'votelabour'], ['disabilities', 'autistic', 'people', 'morally'], ['government', 'extra', 'police', 'resources'], ['impact', 'brexit', 'deal', 'njohnson'], ['johnson', 'nasty', 'piece', 'work'], ['100', 'years', 'ago', 'women'], ['care', 'savaged', 'services', 'privatised'], ['coats', 'floor', 'leeds', 'hospital'], ['continued', 'detention', 'people', 'learning'], ['fees', 'penalised', 'poorest', 'society'], ['industrial', 'relations', 'worse', 'better'], ['botched', 'brexit', 'deal', 'nshe'], ['breach', 'human', 'rights', 'laws'], ['businesses', 'prepare', 'deal', 'brexit'], ['country', 'chaos', 'failure', 'majority'], ['disabled', 'adults', 'pay', 'price'], ['isn', 'willing', 'parliament', 'country'], ['banned', 'banks', 'charging', 'daily'], ['better', 'tories', 'let', 'labour'], ['boris', 'johnson', 'nasty', 'piece'], ['brexit', 'catastrophic', 'nno', 'government'], ['congratulations', 'elected', 'member', 'parliament'], ['good', 'luck', 'today', 'birmingham'], ['grant', 'shapps', 'plans', 'curtail'], ['contained', 'misleading', 'claims', 'period'], ['door', 'statsofshame', 'austerity', 'britain'], ['es', 'crisis', 'waiting', 'lists'], ['heard', 'nthat', 'applying', 'emergency'], ['jobs', 'economy', 'nshe', 'bring'], ['boris', 'johnson', 'commit', 'interview'], ['bring', 'botched', 'deal', 'parliament'], ['child', 'matters', 'na', 'labour'], ['featured', 'misleading', 'factually', 'incorrect'], ['floor', 'leeds', 'hospital', 'nthe'], ['fully', 'prepared', 'bring', 'confidence'], ['applying', 'emergency', 'debate', 'today'], ['breaking', 'news', 'liberal', 'democrat'], ['brexit', 'deal', 'worst', 'worlds'], ['care', 'green', 'paper', 'leaving'], ['case', 'government', 'extra', 'police'], ['clock', 'shameful', 'attempt', 'make'], ['commit', 'interview', 'face', 'questions'], ['crisis', 'waiting', 'lists', 'getting'], ['cut', 'social', 'care', 'savaged'], ['deal', 'ntherefore', 'tabled', 'motion'], ['deal', 'vote', 'week', 'parliament'], ['delayed', 'social', 'care', 'green'], ['deliver', 'meaningful', 'changes', 'brexit'], ['economy', 'nshe', 'bring', 'botched'], ['failed', 'bid', 'deliver', 'meaningful'], ['government', 'falling', 'apart', 'eyes'], ['green', 'paper', 'leaving', 'older'], ['home', 'secretary', 'make', 'case'], ['autistic', 'girl', 'bethany', 'sues'], ['billions', 'pounds', 'asking', 'businesses'], ['brexit', 'deal', 'bad', 'deal'], ['deal', 'look', 'like', 'lesser'], ['deal', 'works', 'country', 'puts'], ['detention', 'people', 'learning', 'disabilities'], ['didn', 'vote', 'theresa', 'bad'], ['extra', 'police', 'resources', 'passing'], ['farage', 'sell', 'nhs', 'nvote'], ['father', 'autistic', 'girl', 'bethany'], ['fork', 'road', 'mighty', 'choice'], ['isn', 'telling', 'truth', 'probably'], ['jack', 'williment', 'barr', 'year'], ['johnson', 'caught', 'lying', 'damaging'], ['johnson', 'commit', 'interview', 'face'], ['asking', 'businesses', 'prepare', 'deal'], ['autistic', 'people', 'morally', 'clinically'], ['banks', 'charging', 'daily', 'monthly'], ['boris', 'johnson', 'tories', 'nigel'], ['brexit', 'deal', 'works', 'country'], ['condemn', 'transport', 'secretary', 'grant'], ['confidence', 'pm', 'evening', 'parliament'], ['constituency', 'parlipuboty', 'view', 'video']]\n",
      "[['year', 'old', 'suspected', 'pneumonia'], ['suspected', 'pneumonia', 'forced', 'lie'], ['old', 'suspected', 'pneumonia', 'forced'], ['theresa', 'botched', 'brexit', 'deal'], ['coats', 'floor', 'leeds', 'hospital'], ['forced', 'lie', 'pile', 'coats'], ['lie', 'pile', 'coats', 'floor'], ['pile', 'coats', 'floor', 'leeds'], ['pneumonia', 'forced', 'lie', 'pile'], ['vote', 'labour', 'generalelection2019', 'votelabourtoday'], ['jack', 'williment', 'barr', 'year'], ['nhs', 'nvote', 'labour', 'thursday'], ['williment', 'barr', 'year', 'old'], ['barr', 'year', 'old', 'suspected'], ['floor', 'leeds', 'hospital', 'nthe'], ['hospital', 'nthe', 'pm', 'grabbed'], ['leeds', 'hospital', 'nthe', 'pm'], ['nthe', 'pm', 'grabbed', 'phone'], ['picture', 'jack', 'williment', 'barr'], ['pm', 'grabbed', 'phone', 'pocket'], ['tried', 'picture', 'jack', 'williment'], ['constituency', 'parlipuboty', 'view', 'video'], ['labour', 'thursday', '12', 'december'], ['parlipuboty', 'view', 'video', 'entry'], ['boris', 'johnson', 'commit', 'interview'], ['challenge', 'boris', 'johnson', 'commit'], ['commit', 'interview', 'face', 'questions'], ['face', 'questions', 'people', 'deemed'], ['interview', 'face', 'questions', 'people'], ['interview', 'prepared', 'oven', 'ready'], ['issues', 'challenge', 'boris', 'johnson'], ['johnson', 'commit', 'interview', 'face'], ['johnson', 'likes', 'say', 'nandrew'], ['late', 'interview', 'prepared', 'oven'], ['likes', 'say', 'nandrew', 'neil'], ['mr', 'johnson', 'likes', 'say'], ['nandrew', 'neil', 'issues', 'challenge'], ['neil', 'issues', 'challenge', 'boris'], ['oven', 'ready', 'mr', 'johnson'], ['prepared', 'oven', 'ready', 'mr'], ['questions', 'people', 'deemed', 'untrustworthy'], ['ready', 'mr', 'johnson', 'likes'], ['say', 'nandrew', 'neil', 'issues'], ['save', 'nhs', 'nvote', 'labour'], ['congratulations', 'elected', 'member', 'parliament'], ['million', 'people', 'poverty', 'uk'], ['14', 'million', 'people', 'poverty'], ['brexit', 'deal', 'works', 'country'], ['just', 'saying', 'said', 'tories'], ['people', 'poverty', 'uk', 'people'], ['said', 'tories', 'state', 'denial'], ['saying', 'said', 'tories', 'state'], ['state', 'denial', 'poverty', 'country'], ['statsofshame', 'austerity', 'britain', 'nsource'], ['tories', 'state', 'denial', 'poverty'], ['denial', 'poverty', 'country', 'nwake'], ['nit', 'just', 'saying', 'said'], ['people', 'nit', 'just', 'saying'], ['poverty', 'country', 'nwake', 'pmqs'], ['poverty', 'uk', 'people', 'nit'], ['uk', 'people', 'nit', 'just'], ['12', 'december', 'save', 'nhs'], ['waiting', 'lists', 'getting', 'longer'], ['child', 'matters', 'na', 'labour'], ['matters', 'na', 'labour', 'government'], ['boris', 'johnson', 'tories', 'nigel'], ['johnson', 'tories', 'nigel', 'farage'], ['labour', 'thursday', 'saveournhs', 'votelabour'], ['nigel', 'farage', 'sell', 'nhs'], ['social', 'care', 'green', 'paper'], ['thursday', '12', 'december', 'save'], ['tories', 'nigel', 'farage', 'sell'], ['unacceptable', 'country', 'wait', 'month'], ['prove', 'boris', 'johnson', 'tories'], ['save', 'nhs', 'votelabour', 'saveournhs'], ['seven', 'things', 'prove', 'boris'], ['things', 'prove', 'boris', 'johnson'], ['vote', 'theresa', 'botched', 'brexit'], ['december', 'save', 'nhs', 'votelabour'], ['door', 'statsofshame', 'austerity', 'britain'], ['end', 'rough', 'sleeping', 'years'], ['nvote', 'labour', 'thursday', '12'], ['vote', 'labour', '12', 'december'], ['100', 'years', 'ago', 'today'], ['brexit', 'deal', 'doesn', 'control'], ['britain', 'theresa', 'brexit', 'deal'], ['nvote', 'labour', 'thursday', 'saveournhs'], ['people', 'britain', 'theresa', 'brexit'], ['theresa', 'brexit', 'deal', 'doesn'], ['working', 'people', 'britain', 'theresa'], ['attorney', 'general', 'legal', 'advice'], ['fully', 'prepared', 'bring', 'confidence'], ['home', 'labour', 'ge2019', 'votelabour'], ['provide', 'free', 'personal', 'care'], ['botched', 'brexit', 'deal', 'nshe'], ['brexit', 'deal', 'bad', 'deal'], ['brexit', 'deal', 'knows', 'lose'], ['brexit', 'deal', 'nshe', 'running'], ['come', 'home', 'labour', 'ge2019'], ['deal', 'nshe', 'running', 'scared']]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Frequencies of all terms\n",
    "all_terms_cons = dict(zip(vec.get_feature_names_out(), totaltf_cons))\n",
    "all_terms_lab = dict(zip(vec.get_feature_names_out(), totaltf_lab))\n",
    "from heapq import nlargest\n",
    "# DICTIONARY - top 100 terms (you can vary this)\n",
    "N = 100\n",
    "top100_terms_cons = nlargest(N, all_terms_cons , key = all_terms_cons.get)\n",
    "top100_terms_lab = nlargest(N, all_terms_lab , key = all_terms_lab.get)\n",
    "#print(top100_terms_cons)\n",
    "#print(top100_terms_lab)\n",
    "\n",
    "# %%\n",
    "# Add column for filtering with text split\n",
    "#demojized_text_split = [i.split() for i in list(uk_data['demojized_text'])]\n",
    "top100_terms_split_cons = [i.split() for i in top100_terms_cons]\n",
    "top100_terms_split_lab = [i.split() for i in top100_terms_lab]\n",
    "print(top100_terms_split_cons)\n",
    "print(top100_terms_split_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2def7f5",
   "metadata": {},
   "source": [
    "That is basically returning two nested lists with the most frequent 4-grams. When going through the list it is clearly visible that most of the 4-grams could be defined as polital spin. Essentially as an approach to make the before used collabsed dictionary more robust, we will substract the words used by the conservatives from the labour term, to get a more labour unique dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2747aa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nsource',\n",
       " 'say',\n",
       " 'uk',\n",
       " 'forced',\n",
       " 'questions',\n",
       " 'saying',\n",
       " 'people',\n",
       " 'working',\n",
       " 'said',\n",
       " 'pneumonia',\n",
       " 'face',\n",
       " 'ge2019',\n",
       " 'neil',\n",
       " 'rough',\n",
       " 'grabbed',\n",
       " 'theresa',\n",
       " 'farage',\n",
       " 'just',\n",
       " 'barr',\n",
       " 'denial',\n",
       " 'hospital',\n",
       " 'nwake',\n",
       " 'unacceptable',\n",
       " 'britain',\n",
       " 'prove',\n",
       " 'things',\n",
       " 'tried',\n",
       " 'phone',\n",
       " 'sell',\n",
       " 'commit',\n",
       " 'come',\n",
       " 'thursday',\n",
       " 'jack',\n",
       " 'years',\n",
       " 'view',\n",
       " 'boris',\n",
       " 'entry',\n",
       " 'save',\n",
       " 'coats',\n",
       " 'tories',\n",
       " 'suspected',\n",
       " 'nit',\n",
       " 'prepared',\n",
       " 'issues',\n",
       " 'williment',\n",
       " 'ready',\n",
       " 'year',\n",
       " 'statsofshame',\n",
       " 'saveournhs',\n",
       " 'votelabour',\n",
       " '12',\n",
       " 'sleeping',\n",
       " 'picture',\n",
       " 'untrustworthy',\n",
       " 'nhs',\n",
       " 'wait',\n",
       " 'likes',\n",
       " 'floor',\n",
       " 'nthe',\n",
       " 'control',\n",
       " 'na',\n",
       " 'pmqs',\n",
       " 'nvote',\n",
       " 'oven',\n",
       " 'doesn',\n",
       " 'end',\n",
       " 'pocket',\n",
       " 'provide',\n",
       " 'state',\n",
       " 'austerity',\n",
       " 'home',\n",
       " 'labour',\n",
       " '14',\n",
       " 'johnson',\n",
       " 'seven',\n",
       " 'free',\n",
       " 'challenge',\n",
       " 'pm',\n",
       " 'mr',\n",
       " 'nigel',\n",
       " 'generalelection2019',\n",
       " 'late',\n",
       " 'running',\n",
       " 'deemed',\n",
       " 'poverty',\n",
       " 'old',\n",
       " 'votelabourtoday',\n",
       " 'parlipuboty',\n",
       " 'video',\n",
       " 'interview',\n",
       " 'nandrew',\n",
       " 'december',\n",
       " 'month',\n",
       " 'matters',\n",
       " 'million',\n",
       " 'lie',\n",
       " 'leeds',\n",
       " 'pile']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## flatten 4-grams and substracting conservatives terms from labour terms\n",
    "flat_list_cons = [item for sublist in top100_terms_split_cons for item in sublist]\n",
    "flat_list_lib = [item for sublist in top100_terms_split_lab for item in sublist]\n",
    "output = [x for x in flat_list_lib if not x in flat_list_cons or flat_list_cons.remove(x)]\n",
    "output = list(set(output))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8b9ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labour_dictionary.txt', 'w') as f:\n",
    "    for item in output:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83171148",
   "metadata": {},
   "source": [
    "### Create a simple measure for spin\n",
    "Basically label data containing spin according to their media attachment. If a video or photo is attached to a tweet from the official party accounts, these tweets can be in general classified as spin. Of course there will be some exceptions, but the idea is to measure differences using this labelling strategy in the regression to previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a324094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_md = pd.read_csv(\"uk_tweets.csv\")\n",
    "uk_md = uk_md[['id', 'created_at', 'entities.urls', 'attachments.media', 'author.public_metrics.followers_count']]\n",
    "\n",
    "uk_md['created_at'] = pd.to_datetime(uk_md['created_at'], format=\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "\n",
    "uk_data_spin = pd.merge(uk_data, uk_md, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21f33d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33138\n",
       "1     7538\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uk_data_spin[uk_data_spin['attachments.media'] != None]\n",
    "#uk_data_spin['label'] = [1 if i.contains('video|photo', na=False) else 0 for i in uk_data_spin['attachments.media']]\n",
    "uk_data_spin['label'] = uk_data_spin['attachments.media'].str.contains('video|photo', na=False).astype(int)\n",
    "uk_data_spin['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
